{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGhdMY1O685k4kAgsRRpGM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"pvE4a4eVYY0-","executionInfo":{"status":"error","timestamp":1723546628055,"user_tz":-330,"elapsed":1226,"user":{"displayName":"Sharath Chandra Devaram","userId":"14083694417890913241"}},"outputId":"b306fb4c-19e3-4e74-84f7-618d715dff2b"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'youtube_transcript_api'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-77db7fa61db9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myoutube_transcript_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYouTubeTranscriptApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'youtube_transcript_api'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import re\n","import csv\n","import pandas as pd\n","from googleapiclient.discovery import build\n","from youtube_transcript_api import YouTubeTranscriptApi"]},{"cell_type":"code","source":["API_KEY = 'Your API Key'"],"metadata":{"id":"BExid00UYkcc","executionInfo":{"status":"ok","timestamp":1723546644744,"user_tz":-330,"elapsed":463,"user":{"displayName":"Sharath Chandra Devaram","userId":"14083694417890913241"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def get_video_id(url):\n","    # extract video id from the URL\n","    video_id_match = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*', url)\n","    return video_id_match.group(1) if video_id_match else None\n","\n","def get_video_title(video_id):\n","    # build the youTube service\n","    youtube = build('youtube', 'v3', developerKey=API_KEY)\n","\n","    # fetch the video details\n","    request = youtube.videos().list(\n","        part='snippet',\n","        id=video_id\n","    )\n","    response = request.execute()\n","\n","    # extract the title\n","    title = response['items'][0]['snippet']['title'] if response['items'] else 'Unknown Title'\n","    return title\n","\n","def get_video_transcript(video_id):\n","    # fetch the transcript\n","    try:\n","        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n","        return transcript\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return []\n","\n","def save_to_csv(title, transcript, filename):\n","    # save the title and transcript to a CSV file\n","    transcript_data = [{'start': entry['start'], 'text': entry['text']} for entry in transcript]\n","    df = pd.DataFrame(transcript_data)\n","    df.to_csv(filename, index=False)\n","\n","    # save the title separately\n","    with open(filename, 'a', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Title:', title])\n","\n","def main():\n","    url = input('Enter the YouTube video link: ')\n","    video_id = get_video_id(url)\n","\n","    if not video_id:\n","        print('Invalid YouTube URL.')\n","        return\n","\n","    title = get_video_title(video_id)\n","    transcript = get_video_transcript(video_id)\n","\n","    if not transcript:\n","        print('No transcript available for this video.')\n","        return\n","\n","    filename = f\"{video_id}_transcript.csv\"\n","    save_to_csv(title, transcript, filename)\n","    print(f'Transcript saved to {filename}')\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"byJXiZpzYqyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","\n","# load the dataset\n","transcript_df = pd.read_csv(\"/content/71op1DQ2gyo_transcript.csv\")\n","print(transcript_df.head())"],"metadata":{"id":"ioZ1ODpMYrr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transcript_df['start'] = pd.to_numeric(transcript_df['start'], errors='coerce')\n","\n","print(\"Dataset Overview:\")\n","print(transcript_df.info())\n","print(\"\\nBasic Statistics:\")\n","print(transcript_df.describe())"],"metadata":{"id":"kn_AfDWvYrup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# distribution of text lengths\n","transcript_df['text_length'] = transcript_df['text'].apply(len)\n","plt.figure(figsize=(10, 5))\n","plt.hist(transcript_df['text_length'], bins=50, color='blue', alpha=0.7)\n","plt.title('Distribution of Text Lengths')\n","plt.xlabel('Text Length')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"Nl2A2VJDYrxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# most common words\n","vectorizer = CountVectorizer(stop_words='english')\n","word_counts = vectorizer.fit_transform(transcript_df['text'])\n","word_counts_df = pd.DataFrame(word_counts.toarray(), columns=vectorizer.get_feature_names_out())\n","common_words = word_counts_df.sum().sort_values(ascending=False).head(20)\n","plt.figure(figsize=(10, 5))\n","common_words.plot(kind='bar', color='green', alpha=0.7)\n","plt.title('Top 20 Common Words')\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"zRwQ7Q6zYr0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# topic Modeling using NMF\n","n_features = 1000\n","n_topics = 10\n","n_top_words = 10\n","\n","tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","tf = tf_vectorizer.fit_transform(transcript_df['text'])\n","nmf = NMF(n_components=n_topics, random_state=42).fit(tf)\n","tf_feature_names = tf_vectorizer.get_feature_names_out()\n","\n","def display_topics(model, feature_names, no_top_words):\n","    topics = []\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topics.append(\" \".join(topic_words))\n","    return topics\n","\n","topics = display_topics(nmf, tf_feature_names, n_top_words)\n","print(\"\\nIdentified Topics:\")\n","for i, topic in enumerate(topics):\n","    print(f\"Topic {i + 1}: {topic}\")"],"metadata":{"id":"4DipLH4ZY19g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get topic distribution for each text segment\n","topic_distribution = nmf.transform(tf)\n","\n","# align the lengths by trimming the extra row in topic_distribution\n","topic_distribution_trimmed = topic_distribution[:len(transcript_df)]\n","\n","# compute the dominant topic for each text segment\n","transcript_df['dominant_topic'] = topic_distribution_trimmed.argmax(axis=1)"],"metadata":{"id":"r9I9kcFRY2AR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyze the content of each text segment to manually identify logical breaks\n","logical_breaks = []\n","\n","for i in range(1, len(transcript_df)):\n","    if transcript_df['dominant_topic'].iloc[i] != transcript_df['dominant_topic'].iloc[i - 1]:\n","        logical_breaks.append(transcript_df['start'].iloc[i])"],"metadata":{"id":"ip-0iXNMY2Cj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# consolidate the logical breaks into broader chapters\n","threshold = 60  # seconds\n","consolidated_breaks = []\n","last_break = None\n","\n","for break_point in logical_breaks:\n","    if last_break is None or break_point - last_break >= threshold:\n","        consolidated_breaks.append(break_point)\n","        last_break = break_point"],"metadata":{"id":"6RLQAGDDY2E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge consecutive breaks with the same dominant topic\n","final_chapters = []\n","last_chapter = (consolidated_breaks[0], transcript_df['dominant_topic'][0])\n","\n","for break_point in consolidated_breaks[1:]:\n","    current_topic = transcript_df[transcript_df['start'] == break_point]['dominant_topic'].values[0]\n","    if current_topic == last_chapter[1]:\n","        last_chapter = (last_chapter[0], current_topic)\n","    else:\n","        final_chapters.append(last_chapter)\n","        last_chapter = (break_point, current_topic)\n","\n","final_chapters.append(last_chapter)  # append the last chapter"],"metadata":{"id":"HcxAxtBDYr3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the final chapters to a readable time format\n","chapter_points = []\n","chapter_names = []\n","\n","for i, (break_point, topic_idx) in enumerate(final_chapters):\n","    chapter_time = pd.to_datetime(break_point, unit='s').strftime('%H:%M:%S')\n","    chapter_points.append(chapter_time)\n","\n","    # get the context for the chapter name\n","    chapter_text = transcript_df[(transcript_df['start'] >= break_point) & (transcript_df['dominant_topic'] == topic_idx)]['text'].str.cat(sep=' ')\n","\n","    # extract key phrases to create a chapter name\n","    vectorizer = TfidfVectorizer(stop_words='english', max_features=3)\n","    tfidf_matrix = vectorizer.fit_transform([chapter_text])\n","    feature_names = vectorizer.get_feature_names_out()\n","    chapter_name = \" \".join(feature_names)\n","\n","    chapter_names.append(f\"Chapter {i+1}: {chapter_name}\")\n","\n","# display the final chapter points with names\n","print(\"\\nFinal Chapter Points with Names:\")\n","for time, name in zip(chapter_points, chapter_names):\n","    print(f\"{time} - {name}\")"],"metadata":{"id":"Elxzrij5ZBrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Doxv_7U4ZBtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PKMbwA1WYr6M"},"execution_count":null,"outputs":[]}]}