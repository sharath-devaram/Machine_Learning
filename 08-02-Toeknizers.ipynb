{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3a575fc5-c5da-41e4-98d4-2f4327666c4d",
   "metadata": {},
   "source": [
    "What is Tokenization?\n",
    "Tokenization is the process of breaking up a piece of text into sentences or words. When we break down textual data into sentences or words, the output we get is known as tokens. There are two strategies for tokenization of a textual dataset:\n",
    "\n",
    "Sentence Tokenization: It means breaking a piece of text into sentences. For example, when you tokenize a paragraph, it splits the paragraph into sentences known as tokens. In many natural language processing problems, splitting text data into sentences is very useful. Sentences are separated by a full stop, so the process of sentence tokenization finds all the full stops in a piece of text to split the data into sentences.\n",
    "\n",
    "Word Tokenization: Word tokenization is the most common way of tokenization. It means to split the complete textual data into words. For example, when you tokenize a paragraph, it splits the paragraph into words known as tokens. Words are separated by a space, so the process of word tokenization finds all the spaces in a piece of text to split the data into words.\n",
    "\n",
    "I hope you now have understood sentence and word tokenization. Now in the section below, I will take you through the implementation of sentence and word tokenization using Python.\n",
    "\n",
    "Sentence and Word Tokenization using Python\n",
    "Sentence tokenization means splitting the textual data into sentences. Here is the implementation of sentence tokenization using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9408c9-c8d7-4dc9-ad4b-6806f1c649bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5c31aa-6d35-4f04-b827-8ea7dbcb6c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6e5c3f-cdd6-40b8-b028-3975364a779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bce0154-77c3-452d-9e8a-08f7a9c34e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hi, My name is Aman, I hope you like my work. You can follow me on Instagram for more resources. My username is 'the.clever.programmer'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2301e5a2-bddc-460d-a697-f0e9ba573b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, My name is Aman, I hope you like my work.', 'You can follow me on Instagram for more resources.', \"My username is 'the.clever.programmer'.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84e19df4-721b-4cd2-8dc1-a2e9573404b1",
   "metadata": {},
   "source": [
    "Word tokenization means splitting the textual data into words. Here is the implementation of word tokenization using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ea7e5b-2d26-4a93-8281-5cd07eebc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d228d4b0-a79d-4c64-8fd6-54c7effd5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b63d62-e5f2-4eac-931f-3b5810063b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'My', 'name', 'is', 'Aman', ',', 'I', 'hope', 'you', 'like', 'my', 'work.', 'You', 'can', 'follow', 'me', 'on', 'Instagram', 'for', 'more', 'resources.', 'My', 'username', 'is', \"'the.clever.programmer\", \"'\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_token.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c98cf-44b6-4ad6-865f-bd0129e410db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
